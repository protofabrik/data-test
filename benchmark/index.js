"'use strict';\nconst { execFileSync } = require('child_process');\nconst clc = require('cli-color');\n\nconst getTrials = (searchTerms) => {\n\t// Without any command-line arguments, we do a general-purpose benchmark.\n\tif (!searchTerms.length) return require('./trials').default;\n\n\t// With command-line arguments, the user can run specific groups of trials.\n\treturn require('./trials').searchable.filter(filterBySearchTerms(searchTerms));\n};\n\nconst filterBySearchTerms = (searchTerms) => (trial) => {\n\tconst terms = [\n\t\ttrial.type,\n\t\ttrial.table,\n\t\t`(${trial.columns.join(', ')})`,\n\t\t`(${trial.columns.join(',')})`,\n\t\t...trial.columns,\n\t\t...trial.customPragma,\n\t];\n\treturn searchTerms.every(arg => terms.includes(arg));\n};\n\nconst sortTrials = (a, b) => {\n\tconst aRo = require(`./types/${a.type}`).readonly;\n\tconst bRo = require(`./types/${b.type}`).readonly;\n\tif (typeof aRo !== 'boolean') throw new TypeError(`Missing readonly export in benchmark type ${a.type}`);\n\tif (typeof bRo !== 'boolean') throw new TypeError(`Missing readonly export in benchmark type ${b.type}`);\n\treturn bRo - aRo;\n};\n\nconst displayTrialName = (trial) => {\n\tif (trial.description) return console.log(clc.magenta(`--- ${trial.description} ---`));\n\tconst name = `${trial.type} ${trial.table} (${trial.columns.join(', ')})`;\n\tconst pragma = trial.customPragma.length ? ` | ${trial.customPragma.join('; ')}` : '';\n\tconsole.log(clc.magenta(name) + clc.yellow(pragma));\n};\n\nconst createContext = (trial, driver) => {\n\tconst tableInfo = Object.assign({}, tables.get(trial.table), { data: undefined });\n\treturn JSON.stringify(Object.assign({}, trial, tableInfo, { driver, tables: [...tables.keys()] }));\n};\n\nconst erase = () => {\n\treturn clc.move(0, -1) + clc.erase.line;\n};\n\n// Determine which trials should be executed.\nprocess.chdir(__dirname);\nconst trials = getTrials(process.argv.slice(2)).sort(sortTrials);\nif (!trials.length) {\n\tconsole.log(clc.yellow('No matching benchmarks found!'));\n\tprocess.exit();\n}\n\n// Create the temporary database needed to run the benchmark trials.\nconsole.log('Generating tables...');\nconst tables = require('./seed')();\nprocess.stdout.write(erase());\n\n// Execute each trial for each available driver.\nconst drivers = require('./drivers');\nconst nameLength = [...drivers.keys()].reduce((m, d) => Math.max(m, d.length), 0);\nfor (const trial of trials) {\n\tdisplayTrialName(trial);\n\tfor (const driver of drivers.keys()) {\n\t\tconst driverName = driver.padEnd(nameLength);\n\t\tconst ctx = createContext(trial, driver);\n\t\tprocess.stdout.write(`${driver} (running...)\\n`);\n\t\ttry {\n\t\t\tconst result = execFileSync('node', ['./benchmark.js', ctx], { stdio: 'pipe', encoding: 'utf8' });\n\t\t\tconsole.log(erase() + `${driverName} x ${result}`);\n\t\t} catch (err) {\n\t\t\tconsole.log(erase() + clc.red(`${driverName} ERROR (probably out of memory)`));\n\t\t\tprocess.stderr.write(clc.xterm(247)(clc.strip(err.stderr)));\n\t\t}\n\t}\n\tconsole.log('');\n}\n\nconsole.log(clc.green('All benchmarks complete!'));\nprocess.exit();\n"